{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfde02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = \"AKIAVJUQMVV33BRDICRG\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"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\"\n",
    "os.environ['AWS_DEFAULT_REGION'] = \"ap-southeast-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89519da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_urimlflow.set_tracking_uri(\"http://ec2-13-211-98-126.ap-southeast-2.compute.amazonaws.com:5000/\")\n",
    "mlflow.set_experiment('Exp6_LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c63a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lightgbm_configs():\n",
    "    \"\"\"\n",
    "        Define different LightGBM configurations to test\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'lgb_light': LGBMClassifier(\n",
    "            n_estimators = 100,\n",
    "            max_depth = 3,\n",
    "            learning_rate = 0.1,\n",
    "            num_leaves = 31,\n",
    "            random_state = 42,\n",
    "            verbose = -1\n",
    "        ),\n",
    "        'lgb_medium': LGBClassifier(\n",
    "            n_estimators = 200,\n",
    "            max_depth = 5,\n",
    "            learning_rate = 0.1,\n",
    "            num_leaves = 63,\n",
    "            random_state = 42,\n",
    "            verbose = -1\n",
    "        ),\n",
    "        'lgb_deep': LGBClassifier(\n",
    "            n_estimators = 300,\n",
    "            max_depth = 7,\n",
    "            learning_rate = 0.1,\n",
    "            num_leaves = 127,\n",
    "            random_state = 42,\n",
    "            verbose = -1\n",
    "        ),\n",
    "        'lgb_conservative': LGBClassifier(\n",
    "            n_estimators = 400,\n",
    "            max_depth = 8,\n",
    "            learning_rate = 0.2,\n",
    "            num_leaves = 255,\n",
    "            subsample = 1.0,\n",
    "            colsample_bytree = 1.0,\n",
    "            random_state = 42,\n",
    "            verbose = -1\n",
    "        ),\n",
    "        'lgb_balanced': LGBClassifier(\n",
    "            n_estimators = 250,\n",
    "            max_depth = 6,\n",
    "            learning_rate = 0.08,\n",
    "            num_leaves = 100,\n",
    "            subsample = 0.9,\n",
    "            colsample_bytree = 0.9,\n",
    "            reg_alpha = 0.05,\n",
    "            reg_lambda = 0.05,\n",
    "            random_state = 42,\n",
    "            verbose = -1\n",
    "        )\n",
    "    }\n",
    "\n",
    "def get_vectorizer_configs():\n",
    "    '''\n",
    "        Define different vectorization strategies \n",
    "    '''\n",
    "    return  {\n",
    "        'tfidf_unigram_10000': TfidfVectorizer(ngram_range(1, 1), \n",
    "                                max_features = 10000),\n",
    "        'tfidf_bigram_10000': TfidfVectorizer(ngram_range(1, 2), \n",
    "                            max_features = 10000),\n",
    "        'tfidf_trigram_10000': TfidfVectorizer(ngram_range(1, 3), \n",
    "                                max_features = 10000),\n",
    "        'tfidf_unigram_5000': TfidfVectorizer(ngram_range(1, 1),\n",
    "                                max_features = 5000),\n",
    "        'tfidf_bigram_5000': TfidfVectorizer(ngram_range(1, 2),\n",
    "                                max_features = 5000),\n",
    "        'tfidf_trigram_5000': TfidfVectorizer(ngram_range(1, 3),\n",
    "                                max_features = 5000),\n",
    "        'bow_bigram_5000': CountVectorizer(ngram_range=(1, 2),\n",
    "                                max_features = 5000),\n",
    "        'bow_trigram_10000': CountVectorizer(ngram_range = (1, 3),\n",
    "                                max_features = 10000)\n",
    "    }\n",
    "\n",
    "def get_imbalance_methods():\n",
    "    '''\n",
    "        Define imabalnce handling techniques that work well with LightGBM\n",
    "    '''\n",
    "    return ['none', 'smote', 'adasyn', 'undersampling', 'smote_enn']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da836cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lightgbm_experiment(lgb_name, \n",
    "                            lgb_model, \n",
    "                            vectorizer_name,\n",
    "                            vectorizer_model,\n",
    "                            imbalance_method):\n",
    "    '''\n",
    "    \n",
    "        Run single LightGBM experiment with given configuration\n",
    "        \n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['clean_comment'],\n",
    "                                                        df['category'],\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 42,\n",
    "                                                        stratify = df['category'])\n",
    "\n",
    "    # Vectorize input\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    # Handle imbalance\n",
    "    if imbalance_method == 'smote':\n",
    "        smote = SMOTE(random_state = 42)\n",
    "        X_train_vec, y_train = smote.fit_resample(X_train_vec, y_train)\n",
    "    elif imbalance_method == 'adasyn':\n",
    "        adasyn = ADASYN(random_state = 42)\n",
    "        X_train_vec, y_train = adasyn.fit_resample(X_train_vec, y_train)\n",
    "    elif imbalance_method == 'undersampling':\n",
    "        rud = RandomUnderSampler(random_state = 42)\n",
    "        X_train_vec, y_train = rud.fit_resample(X_train_vec, y_train)\n",
    "    elif imabalnce_method == 'smote_enn':\n",
    "        smote_enn = SMOTEENN(random_state = 42)\n",
    "        X_train_vec, y_train = smote_enn.fit_resample(X_train_vec, y_train)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb_model.fit(X_train_vec, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_vec)\n",
    "    y_pred_test = model.predict(X_test_vec)\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    precision_weighted = precision_score(y_test, y_pred_test, average = 'weighted')\n",
    "    recall_weighted = recall_score(y_test, y_pred_test, average = 'weighted')\n",
    "    precision_macro = precision_score(y_test, y_pred_test, average = 'macro')\n",
    "    recall_score = recall_score(y_test, y_pred_test, average = 'macro')\n",
    "\n",
    "    # ML Flow logging\n",
    "    with mlflow.start_run() as run:\n",
    "        # Tags\n",
    "        mlflow.set_tag('mlflow.runName', f'LightGBM_{lgb_name}_{vectorizer_name}_{imbalance_method}')\n",
    "        mlflow.set_tag('model_type', 'LightGBM')\n",
    "        mlflow.set_tag('lgb_config', lgb_name)\n",
    "        mlflow.set_tag('vectorizer_type', vectorizer_name)\n",
    "        mlflow.set_tag('imbalance_method', imbalance_method)\n",
    "        mlflow.set_tag('experiment_type', 'lightgbm_optimization')\n",
    "\n",
    "        # Params\n",
    "        mlflow.log_param('lgb_config', lgb_name)\n",
    "        mlflow.log_param('vectorizer_name', vectorizer_name)\n",
    "        mlflow.log_param('imbalance_method', imbalance_method)\n",
    "        mlflow.log_param('ngram_range', str(vectorizer.ngram_range))\n",
    "        mlflow.log_param('vectorizer_max_features', vectorizer.max_features)\n",
    "        \n",
    "        # Parameters model\n",
    "        mlflow.log_param('n_estimators', lgb_model.n_estimators)\n",
    "        mlflow.log_param('max_depth', lgb_model.max_depth)\n",
    "        mlflow.log_param('learning_rate', lgb_model.learning_rate)\n",
    "        mlflow.log_param('num_leaves', lgb_model.num_leaves)\n",
    "        if hasattr(lgb_model, 'subsample'):\n",
    "            mlflow.log_param('subsample', lgb_model.subsample)\n",
    "        if hasattr(lgb_model, 'colsample_bytree'):\n",
    "            mlflow.log_param('colsample_bytree', lgb_model.colsample_bytree)\n",
    "        if hasattr(lgb_model, 'reg_alpha'):\n",
    "            mlflow.log_param('reg_alpha', lgb_model.reg_alpha)\n",
    "        if hasattr(lgb_model, 'reg_lambda'):\n",
    "            mlflow.log_param('reg_lambda'. lgb_model.reg_lambda)\n",
    "        \n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric('train_accuracy', train_accuracy)\n",
    "        mlflow.log_metric('test_accuracy', test_accuracy)\n",
    "        mlflow.log_metric('precision_weighted', precision_weighted)\n",
    "        mlflow.log_metric('recall_weighted', recall_weighted)\n",
    "        mlflow.log_metric('precision_macro', precision_macro)\n",
    "        mlflow.log_metric('recall_macro', recall_macro)\n",
    "\n",
    "        class_rep = classification_report(y_test, y_pred_test, output_dict = True)\n",
    "        for label, metrics in class_report.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric, value in metrics.items():\n",
    "                    mlflow.log_metric(f'{label}_{metric}', value)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred_test)\n",
    "        plt.figure(figsize = (10, 8))\n",
    "        sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues')\n",
    "        plt.title(f'LightGBM Confusion Matrix: {lgb_name}_{vectorizer_name}_{imbalance_method}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "\n",
    "        filename = f'lgb_confusion_matrix_{lgb_name}_{vectorizer_name}_{imbalance_method}.png'\n",
    "        plt.savefig(filename)\n",
    "        mlflow.log_artifact(filename)\n",
    "        plt.close()\n",
    "\n",
    "        # Feature importance top 20\n",
    "        if hasattr(lgb_model, 'feature_importances_'):\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names, \n",
    "                'importance': lgb_model.feature_importances_\n",
    "            }).sort_values('importance', ascending = False).head(20)\n",
    "\n",
    "            plt.figure(figsize = (10, 8))\n",
    "            sns.barplot(data=importance_df, x = 'importance', y = 'feature')\n",
    "            plt.title(f'Top 20 feature importance = {lgb_name}')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            importance_filename = f'importance_{lgb_name}_{vectorizer_name}_{imbalance_method}.png'\n",
    "            plt.savefig(importance_filename)\n",
    "            mlflow.log_artifact(importance_filename)\n",
    "            plt.close()\n",
    "\n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            lgb_model, \n",
    "            name = f'lgb_model_{lgb_name}_{vectorizer_name}_{imbalance_method}',\n",
    "            registred_model = f'reg_lgb_model_{lgb_name}_{vectorizer_name}_{imbalance_method}'\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605261a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_experiment():\n",
    "    '''\n",
    "        Run comprehensive LightGBM experiments with all combinations\n",
    "    '''\n",
    "    lgb_config = get_lightgbm_configs()\n",
    "    vectorizers = get_vectorizer_configs()\n",
    "    imbalance_methods = get_imbalance_methods()\n",
    "\n",
    "    results = []\n",
    "    total_experiments = len(lgb_configs) * len(vectorizers) * len(imbalance_methods)\n",
    "    current_exp = 0\n",
    "\n",
    "    print(f'Starting {total_experiments} comprehensive LightGBM experiments')\n",
    "\n",
    "    for lgb_name, lgb_model in lgb_config.items():\n",
    "        for vec_name, vectorizer in vectorizers.items():\n",
    "            for imbalance_method in imbalance_methods:\n",
    "                current_exp += 1\n",
    "                print(f'Light GBM experiment {current_exp}/{total_experiments}')\n",
    "                try:\n",
    "                    result = run_lightgbm_experiment(lgb_name, \n",
    "                                                    lgb_model,\n",
    "                                                    vec_name,\n",
    "                                                    vectorizer, \n",
    "                                                    imbalance_method)\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f'error - {e}')\n",
    "                    continue\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac68519",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_full_results = run_full_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_sentiment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
